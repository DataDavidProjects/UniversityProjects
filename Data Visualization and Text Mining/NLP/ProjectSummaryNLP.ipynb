{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3a1102",
   "metadata": {},
   "source": [
    "# 20 NewsGroup NLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "from string import punctuation\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder , StandardScaler , MaxAbsScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras import Sequential\n",
    "from keras import metrics\n",
    "\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    # print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def timing(f):\n",
    "    def wrap(*args, **kwargs):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args, **kwargs)\n",
    "        time2 = time.time()\n",
    "        print('{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0))\n",
    "\n",
    "        return ret\n",
    "    return wrap\n",
    "\n",
    "def create_model(optimizer=\"adam\",\n",
    "                 dense_layer_sizes = False,\n",
    "                 dropout=0.1, init='uniform',\n",
    "                 features=3000,neurons=20,\n",
    "                 n_classes = 8 ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(features,),kernel_initializer=init)) #\n",
    "    model.add(Dropout(dropout), )    \n",
    "\n",
    "    #for layer_size in dense_layer_sizes:\n",
    "    #   model.add(Dense(layer_size, activation='relu'))\n",
    "    #   model.add(Dropout(dropout), )    \n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax')) # because we want the probability of each class as output , len 8 \n",
    "    model.compile(loss='sparse_categorical_crossentropy', # sparse because it can accept the integer cat as y . len1 \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def downsample(df):\n",
    "    minority_frequency  = df['label'].value_counts()[-1]\n",
    "    minority_label = df['label'].value_counts().index[-1]\n",
    "    \n",
    "    df_balanced = df.loc[df['label'] == minority_label , : ].sample(minority_frequency).copy()\n",
    "    df_balanced = df_balanced.reset_index(drop = True)\n",
    "    \n",
    "    label_list = df['label'].value_counts().index.tolist()\n",
    "    #Sample and concat\n",
    "    for label in label_list:\n",
    "        if label != minority_label:\n",
    "            sample_df = df.loc[df['label'] == label , : ].sample(minority_frequency).copy()\n",
    "            df_balanced = pd.concat([ df_balanced , sample_df],axis = 0 , ignore_index=True) \n",
    "    # Shuffle data\n",
    "    df_balanced = df_balanced.sample(frac = 1).reset_index(drop = True)\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "    \n",
    "#Need OPT\n",
    "def spacy_preprocessing(text_format):\n",
    "        \n",
    "    def combine_text(list_of_text):\n",
    "            combined_text = ' '.join(list_of_text)\n",
    "            return combined_text\n",
    "        \n",
    "    doc  = nlp(text_format)\n",
    "    tokens_list  = [ token for token in doc if not token.is_punct and not token.is_space and token.is_alpha]\n",
    "    filter_token_sw = [token.lemma_ for token in tokens_list if str.lower(str.strip(token.lemma_)) not in stopwords.words('english')]\n",
    "   \n",
    "    return combine_text(filter_token_sw)\n",
    "\n",
    "def word_freq(label): return pd.Series([ t for t in nlp(grouped_vocabolary[label])] ).value_counts()\n",
    "\n",
    "vec_prop = np.vectorize(spacy_preprocessing)\n",
    "pipe_spacy_preprocessing = FunctionTransformer(vec_prop)\n",
    "prep_pipeline = Pipeline([\n",
    "                    ('text_preprocessing', pipe_spacy_preprocessing )\n",
    "                    ])\n",
    "\n",
    "\n",
    "def make_custom_predictions(fitted_pipe):\n",
    "    prep_text = pipe_spacy_preprocessing.transform( pd.Series(input('Input-Text:')))\n",
    "    return label_encoder.inverse_transform( fitted_pipe.predict(prep_text))\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d1092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________________________________________________ DATA INGESTION___________________________________________________________________\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++Macro Categories++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "cat_dict = {\n",
    "**dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "**dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "**dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "**dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "**dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "**dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "**dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "**dict.fromkeys( ['sci.med'],'medicine') \n",
    "}\n",
    "df['label']=df['label'].map(cat_dict)\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Encode categorical variables in integer levels \n",
    "label_encoder = LabelEncoder()  \n",
    "# Encode labels in column \n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "#SHuffle\n",
    "df = df.sample(frac = 1)\n",
    "# DROPNAN\n",
    "df = df.dropna()\n",
    "# dependent and independent variable\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "#_____________________________________________________________________________________________________________________________________________\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce008ec7",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample dataset to make it balanced without changing the data and make eda faster\n",
    "df_balanced = downsample(df)\n",
    "\n",
    "# Create Tokens and text as columns\n",
    "df_s = df_balanced.copy()\n",
    "\n",
    "#+++++++++++++++++++++++ Create Token and clean them ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "df_s['tokens'] = df_s['text'].apply( lambda x : [token for token in nlp(x) if ( not token.is_punct and \\\n",
    "                                                                                not token.is_space  and \\\n",
    "                                                                                token.is_alpha and \\\n",
    "                                                                                str.lower(str.strip(token.lemma_)) not in stopwords.words('english')  ) ] )\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "df_s['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s['lemma_'] = df_s['tokens'].apply(lambda x: [ str.strip(token.lemma_) for token in  x  ] )\n",
    "\n",
    "# Length of the list of lemma in text\n",
    "df_s['len_lemma_text'] = df_s['lemma_'].apply(lambda x: len(x) )\n",
    "# Unique words in lemma\n",
    "df_s['lemma_unique'] = df_s['lemma_'].apply(lambda x: list(set(x)) ) \n",
    "# Lenght of Unique words in lemma\n",
    "df_s['len_lemma_unique_text'] = df_s['lemma_unique'].apply(lambda x:len( set(x) ) )\n",
    "# Proportion of unique lemma on the lenght of NON unique lemma , the lower the value the more the words in text are repeated\n",
    "df_s['richness_text'] = df_s['len_lemma_unique_text']/df_s['len_lemma_text'] \n",
    "\n",
    "# Drop NAN if text was empty after tokenization\n",
    "df_s = df_s.loc[ df_s['len_lemma_text'] != 0  , : ]\n",
    "\n",
    "# Balance again \n",
    "df_s = downsample(df_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c80a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.suptitle('Distribution of Text Richness per category',fontsize=25)\n",
    "for n,i in enumerate( df_s['label'].unique() ):\n",
    "    ax = fig.add_subplot(3,4,n+1)\n",
    "    abs_freq = df_s.loc[ df_s['label'] == i,'richness_text'] \n",
    "    (abs_freq).hist(ax  = ax )\n",
    "    values = df_s.loc[ df_s['label'] == i,'richness_text']\n",
    "    ax.set_title(str.upper(i) + f' skewness: {round(stats.skew(values),3)  } ' )\n",
    "#     ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d525f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "group_class_docs = df_s.groupby('label')['lemma_unique'].apply(list) \n",
    "group_class_uniqe_texts = group_class_docs.apply(lambda x : list(itertools.chain(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabolary_len_per_category = pd.Series({cat :len(group_class_uniqe_texts.apply(lambda x:list(set(x)))[cat]) for cat in df_s.label.unique() })\n",
    "vocabolary_len_per_category.sort_values().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad63c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(vocabolary_len_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_vocabolary = group_class_uniqe_texts.apply(lambda x:str(' '.join(x)))\n",
    "vocabulary_len_category = dict({ k:len(word_freq(k).index.to_list()) for k in grouped_vocabolary.index.to_list() } )\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for n,i in enumerate(grouped_vocabolary.index.to_list()):\n",
    "    ax = fig.add_subplot(4,2,n+1)\n",
    "    wordcloud = WordCloud(max_words=3000,background_color=\"white\").generate(str(grouped_vocabolary[i]))\n",
    "    ax.set_title(str.upper(i) ) \n",
    "    ax.axis('off')\n",
    "    ax.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ token for token in nlp(df_s.loc[0,'text']) if ( not token.is_punct and \\\n",
    "                                          not token.is_space  and \\\n",
    "                                          token.is_alpha and \\\n",
    "                                          str.lower(str.strip(token.lemma_)) not in stopwords.words('english')  ) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7bb61",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_prop = np.vectorize(spacy_preprocessing)\n",
    "pipe_spacy_preprocessing = FunctionTransformer(vec_prop)\n",
    "\n",
    "prep_pipeline = Pipeline([\n",
    "                    ('text_preprocessing', pipe_spacy_preprocessing )\n",
    "                    ])\n",
    "\n",
    "# DEFINE LABELS IN OHE FORMAT\n",
    "yc = tf.keras.utils.to_categorical(y,num_classes = 8,dtype=int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70189bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++ Preprocessing informative texts balances ++++++++++++++++++++++++++++++++++++\n",
    "X = df_s['text']\n",
    "y = df_s['target']\n",
    "\n",
    "# Transform text\n",
    "X_t = prep_pipeline.fit_transform(X)\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f3dc7",
   "metadata": {},
   "source": [
    "# Model Pipeline- Approach TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f066685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++++++ Pipeline Defition and Params +++++++++++++++++++++++++++++++++\n",
    "\n",
    "# TF IDF DIMENSION will affect the model \n",
    "matrix_features  = 6000 # may it be an approximation of the min len of vocabolary per each category\n",
    "\n",
    "n_classes =y.nunique()\n",
    "\n",
    "#dense_layer_sizes = [[] , []]\n",
    "#param_grid = dict(neurons=neurons, epochs = epochs, batch_size =batch_size)\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'kc__epochs': [20,30,50],\n",
    "    'kc__neurons': [10, 20, 30, 100],\n",
    "    'kc__batch_size':[16, 32,50],\n",
    "    'kc__dropout': [ 0.3, 0.1, 0]\n",
    "}\n",
    "\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(use_idf = True,max_features=matrix_features)),\n",
    "                    ('sparse_to_dense',DenseTransformer()),\n",
    "                    ('scaler', MaxAbsScaler()),\n",
    "                    ('kc' ,KerasClassifier(build_fn=create_model, verbose = 0))\n",
    "])\n",
    "\n",
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "grid = GridSearchCV(estimator=model_pipeline,\n",
    "                    verbose=1,\n",
    "                    cv=skf.split(X_t,y),\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy')\n",
    "\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75e8e8",
   "metadata": {},
   "source": [
    "## Hyperparameters-tuning  and CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae87fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "#++++++++++++++++++++++++++++++ RUN GRID ++++++++++++++++++++++++++++++++++++\n",
    "t0 = time.time()\n",
    "grid_fitted = grid.fit(X_t,y) # Pipe line fitted with preprocessed clean text spacy\n",
    "results  =  pd.DataFrame(grid_fitted.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "t1 = time.time()\n",
    "delta = t1-t0\n",
    "print(f'Tuning Time s: {round(delta,3)}')\n",
    "display( results.head() ) \n",
    "\n",
    "\n",
    "#+++++++++++++++++++++++ BEST PIPE PARAMS ++++++++++++++++++++++++++++++++\n",
    "opt_pipeline  = grid_fitted.best_estimator_\n",
    "\n",
    "t0 = time.time()\n",
    "fitted_pipe = opt_pipeline.fit(X_t,y)\n",
    "time.time() - t0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ded1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keras_pipe(pipeline,name_model = f'keras_model_{matrix_features}.h5', name_pipe =f'sklearn_pipeline_{matrix_features}.pkl' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015830ec",
   "metadata": {},
   "source": [
    "# Model Pipeline- Approach Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7baadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "# load embeddings model from Tensorflow Hub\n",
    "#https://stackoverflow.com/questions/62464152/universal-sentence-encoder-load-error-error-savedmodel-file-does-not-exist-at\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "\n",
    "matrix_features  = 512\n",
    "def create_model(optimizer=\"adam\",\n",
    "                 dense_layer_sizes = False,\n",
    "                 dropout=0.1, init='uniform',\n",
    "                 features=matrix_features,neurons=20,\n",
    "                 n_classes = n_classes ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(features,),kernel_initializer=init)) #\n",
    "    model.add(Dropout(dropout), )    \n",
    "\n",
    "    #for layer_size in dense_layer_sizes:\n",
    "    #   model.add(Dense(layer_size, activation='relu'))\n",
    "    #   model.add(Dropout(dropout), )    \n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "n_classes =y.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46103c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return embed(X).numpy()\n",
    "    \n",
    "emb_pipeline = Pipeline([\n",
    "                    ('embed', Embedding()),\n",
    "                    ('kc' ,KerasClassifier(build_fn=create_model, verbose = 0))\n",
    "])\n",
    "\n",
    "#++++++++++++++++++++++++++++++ GRID ++++++++++++++++++++++++++++++++++++++\n",
    "param_grid = {\n",
    "    'kc__epochs': [20,30,50],\n",
    "    'kc__neurons': [10, 20, 30, 100],\n",
    "    'kc__batch_size':[16, 32,50],\n",
    "    'kc__dropout': [ 0.3, 0.1, 0]\n",
    "}\n",
    "\n",
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "grid = GridSearchCV(estimator=emb_pipeline,\n",
    "                    verbose=1,\n",
    "                    cv=skf.split(X_t,y),\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "t0 = time.time()\n",
    "grid_fitted = grid.fit(X_t,y) # Pipe line fitted with preprocessed clean text spacy\n",
    "results  =  pd.DataFrame(grid_fitted.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "t1 = time.time()\n",
    "delta = t1-t0\n",
    "print(f'Tuning Time s: {round(delta,3)}')\n",
    "display( results.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++++++++++ Fit the best model for embedding ++++++++++++++++++++++++++++++++\n",
    "emb_class = Embedding().fit(X_t)\n",
    "keras_model_emb = create_model(features=512, neurons=30, n_classes=8, dropout=0.3).fit(emb_class.transform(X_t),\n",
    "                                                                                       y,\n",
    "                                                                                       batch_size = 30,\n",
    "                                                                                       epochs = 50 ,\n",
    "                                                                                       verbose= 0  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d29f53",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++Macro Categories++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "cat_dict = {\n",
    "**dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "**dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "**dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "**dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "**dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "**dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "**dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "**dict.fromkeys( ['sci.med'],'medicine') \n",
    "}\n",
    "df['label']=df['label'].map(cat_dict)\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "# Encode labels in column 'species'.\n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# drop non in categories\n",
    "df = df.loc[df['label'].isin(key_categories)]\n",
    "#++++++++++++++++++++++++ PICK RANDOM 30 % OF TEST++++++++++++++++++++++++++\n",
    "df = df.sample(frac = 1) \n",
    "# dependent and independent variable\n",
    "X_test = df['text']\n",
    "y_test = df['target']\n",
    "#_____________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cbab5",
   "metadata": {},
   "source": [
    "### Load Models and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = prep_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline first:\n",
    "chosen_pipe = 'sklearn_pipeline_6000.pkl'\n",
    "pipeline = joblib.load(chosen_pipe)\n",
    "# Then, load the Keras model:\n",
    "\n",
    "chosen_model = 'keras_model_6000.h5'\n",
    "pipeline.named_steps['kc'].model = load_model(chosen_model) \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#X_test_prep = prep_pipeline.transform(X_test)\n",
    "y_pred  = pipeline.predict(X_test_prep)\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(14, 12, forward=True)\n",
    "#fig.align_labels()\n",
    "\n",
    "label_names = df['label'].unique()\n",
    "\n",
    "print(classification_report(y_test,y_pred, target_names=label_names))\n",
    "# fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.asarray(label_names), normalize=False,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax( keras_model_emb.model.predict( emb_class.transform(X_test_prep) )  , axis=-1 ) \n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(14, 12, forward=True)\n",
    "#fig.align_labels()\n",
    "\n",
    "label_names = df['label'].unique()\n",
    "\n",
    "print(classification_report(y_test,y_pred, target_names=label_names))\n",
    "# fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.asarray(label_names), normalize=False,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95c50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
