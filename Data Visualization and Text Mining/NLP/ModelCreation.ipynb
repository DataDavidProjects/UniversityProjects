{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81e2367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "from string import punctuation\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder , StandardScaler , MaxAbsScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras import Sequential\n",
    "from keras import metrics\n",
    "\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def timing(f):\n",
    "    def wrap(*args, **kwargs):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args, **kwargs)\n",
    "        time2 = time.time()\n",
    "        print('{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0))\n",
    "\n",
    "        return ret\n",
    "    return wrap\n",
    "\n",
    "\n",
    "\n",
    "def create_model(optimizer=\"adam\",\n",
    "                 dense_layer_sizes = False,\n",
    "                 dropout=0.1, init='uniform',\n",
    "                 features=3000,neurons=20,\n",
    "                 n_classes = 8 ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(features,),kernel_initializer=init)) #\n",
    "    model.add(Dropout(dropout), )    \n",
    "\n",
    "    #for layer_size in dense_layer_sizes:\n",
    "    #   model.add(Dense(layer_size, activation='relu'))\n",
    "    #   model.add(Dropout(dropout), )    \n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['sparse_categorical_crossentropy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Need OPT\n",
    "def spacy_preprocessing(text_format):\n",
    "        \n",
    "    def combine_text(list_of_text):\n",
    "            combined_text = ' '.join(list_of_text)\n",
    "            return combined_text\n",
    "        \n",
    "    doc  = nlp(text_format)\n",
    "    tokens_list  = [ token for token in doc if not token.is_punct and not token.is_space and token.is_alpha]\n",
    "    filter_token_sw = [token.lemma_ for token in tokens_list if str.lower(str.strip(token.lemma_)) not in stopwords.words('english')]\n",
    "   \n",
    "    return combine_text(filter_token_sw)\n",
    "\n",
    "vec_prop = np.vectorize(spacy_preprocessing)\n",
    "pipe_spacy_preprocessing = FunctionTransformer(vec_prop)\n",
    "\n",
    "\n",
    "prep_pipeline = Pipeline([\n",
    "                    ('text_preprocessing', pipe_spacy_preprocessing )\n",
    "                    ])\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from collections import Counter\n",
    "\n",
    "wc = WordCloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a4e6b",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c91b22d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer      2936\n",
       "science       1779\n",
       "politics      1575\n",
       "sport         1197\n",
       "automobile    1192\n",
       "religion       976\n",
       "medicine       594\n",
       "sales          585\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#______________________________________________________ DATA INGESTION___________________________________________________________________\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++Macro Categories++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "cat_dict = {\n",
    "**dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "**dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "**dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "**dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "**dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "**dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "**dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "**dict.fromkeys( ['sci.med'],'medicine') \n",
    "}\n",
    "df['label']=df['label'].map(cat_dict)\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "# Encode labels in column \n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "df = df.sample(frac = 1)\n",
    "# dependent and independent variable\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "df['label'].value_counts()\n",
    "#_____________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5574a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d29181",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c45518",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580be2db",
   "metadata": {},
   "source": [
    "### Sample for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df.sample(5000)\n",
    "df_s['spacy_obj'] = df_s['text'].apply(lambda x:nlp(x))\n",
    "df_s['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_s.sample(1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2377cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.text.values[0] ,sample.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([token.text for token in nlp(sample.text.values[0]) if token.is_alpha ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([token.text for token in nlp(sample.text.values[0]) if token.is_alpha ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de760afc",
   "metadata": {},
   "source": [
    "## Example of Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdba695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s['tokens'] =\n",
    "df_s['tokens'] = df_s['spacy_obj'].apply(lambda x:list(x)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s['is_alpha_pure'] = df_s['tokens'].apply(lambda x: [token.text for token in x if  token.is_alpha ] )\n",
    "df_s['is_alpha_num'] = df_s['tokens'].apply(lambda x: [token.text for token in x if  not token.is_alpha ] )\n",
    "df_s['len_text'] = df_s['text'].apply(lambda x: len(x) )\n",
    "df_s['unique_text'] = df_s['is_alpha_pure'].apply(lambda x: list(set(x)) ) \n",
    "df_s['n_unique_text'] = df_s['is_alpha_pure'].apply(lambda x:len( set(x) ) )\n",
    "df_s['richness_text'] = df_s['n_unique_text']/df_s['len_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d413bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s['richness_text'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577273d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "group_class_docs = df_s.groupby('label')['unique_text'].apply(list) \n",
    "group_class_uniqe_texts = group_class_docs.apply(lambda x : list(itertools.chain(*x)))\n",
    "group_class_uniqe_texts.apply(lambda x:len(x)) \n",
    "grouped_vocabolary = group_class_uniqe_texts.apply(lambda x:str(' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(label): return pd.Series([ t.lemma_ for t in nlp(grouped_vocabolary[label]) if str.lower(str.strip(t.lemma_)) not in stopwords.words('english')  ] ).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_len_category = dict({ k:len(word_freq(k).index.to_list()) for k in grouped_vocabolary.index.to_list() } )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4391288",
   "metadata": {},
   "source": [
    "## We decide the size of sparse matrix tfidf based on the median of the vocabs per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3410e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(vocabulary_len_category).sort_values().describe()#  be aware that the smaple might be biased "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f73d30",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc  = nlp('I  @  1223 am happy for your new promotion')\n",
    "# tokens_list  = [ token for token in doc if not token.is_punct and not token.is_space and token.is_alpha]\n",
    "# filter_token_sw = [token.lemma_ for token in tokens_list if token.lower_ not in stopwords.words('english')]\n",
    "# filter_token_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a15b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of labels\n",
    "pd.DataFrame(y.value_counts()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb564b",
   "metadata": {},
   "source": [
    "### Downsampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a93fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df):\n",
    "    minority_frequency  = df['label'].value_counts()[-1]\n",
    "    minority_label = df['label'].value_counts().index[-1]\n",
    "    \n",
    "    df_balanced = df.loc[df['label'] == minority_label , : ].sample(minority_frequency).copy()\n",
    "    df_balanced = df_balanced.reset_index(drop = True)\n",
    "    \n",
    "    label_list = df['label'].value_counts().index.tolist()\n",
    "    #Sample and concat\n",
    "    for label in label_list:\n",
    "        if label != minority_label:\n",
    "            sample_df = df.loc[df['label'] == label , : ].sample(minority_frequency).copy()\n",
    "            df_balanced = pd.concat([ df_balanced , sample_df],axis = 0 , ignore_index=True) \n",
    "    # Shuffle data\n",
    "    df_balanced = df_balanced.sample(frac = 1).reset_index(drop = True)\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "df_balanced = downsample(df)\n",
    "# dependent and independent variable\n",
    "X = df_balanced['text']\n",
    "y = df_balanced['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93a1a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "vec_prop = np.vectorize(spacy_preprocessing)\n",
    "pipe_spacy_preprocessing = FunctionTransformer(vec_prop)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "prep_pipeline = Pipeline([\n",
    "                    ('text_preprocessing', pipe_spacy_preprocessing )\n",
    "                    ])\n",
    "\n",
    "# DEFINE LABELS IN OHE FORMAT\n",
    "yc = tf.keras.utils.to_categorical(y,num_classes = 20,dtype=int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d6c6f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4680,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21a0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e77044f0",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6198b3",
   "metadata": {},
   "source": [
    "### Hyperparameters fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4067fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF DIMENSION will affect the model \n",
    "matrix_features  = 3000 # may it be an approximation of the len of vocabolary per each category\n",
    "\n",
    "n_classes =y.nunique()\n",
    "\n",
    "#dense_layer_sizes = [[] , []]\n",
    "#param_grid = dict(neurons=neurons, epochs = epochs, batch_size =batch_size)\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'kc__epochs': [20,30,50],\n",
    "    'kc__neurons': [10, 20, 30, 100],\n",
    "    'kc__batch_size':[16, 32,50],\n",
    "    'kc__dropout': [ 0.3, 0.1, 0]\n",
    "}\n",
    "\n",
    "\n",
    "#StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29379166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_2356/1337535005.py:5: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  ('kc' ,KerasClassifier(build_fn=create_model, verbose = 0))\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(use_idf = True,max_features=matrix_features)),\n",
    "                    ('sparse_to_dense',DenseTransformer()),\n",
    "                    ('scaler', MaxAbsScaler()),\n",
    "                    ('kc' ,KerasClassifier(build_fn=create_model, verbose = 0))\n",
    "])\n",
    "\n",
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "grid = GridSearchCV(estimator=model_pipeline,\n",
    "                    verbose=1,\n",
    "                    cv=skf.split(X_t,y),\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "480d6bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 {color: black;background-color: white;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 pre{padding: 0;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-toggleable {background-color: white;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-estimator:hover {background-color: #d4ebff;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-item {z-index: 1;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-parallel-item:only-child::after {width: 0;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-8281b4a5-6ccb-48f2-89f6-4fc137084409 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-8281b4a5-6ccb-48f2-89f6-4fc137084409\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"65d69279-071b-4f41-9f2a-683064b39ca0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"65d69279-071b-4f41-9f2a-683064b39ca0\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=<generator object _BaseKFold.split at 0x000001370D0CD660>,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(max_features=3000)),\n",
       "                                       ('sparse_to_dense',\n",
       "                                        <__main__.DenseTransformer object at 0x00000137F508FBE0>),\n",
       "                                       ('scaler', MaxAbsScaler()),\n",
       "                                       ('kc',\n",
       "                                        <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000137F508FFA0>)]),\n",
       "             param_grid={'kc__batch_size': [16, 32, 50],\n",
       "                         'kc__dropout': [0.3, 0.1, 0],\n",
       "                         'kc__epochs': [20, 30, 50],\n",
       "                         'kc__neurons': [10, 20, 30, 100],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='accuracy', verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"dbc79824-5c46-41ad-ab4f-1428f3bbead4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"dbc79824-5c46-41ad-ab4f-1428f3bbead4\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=3000)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6af1d908-3c53-438b-bf71-023b6ace7f49\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6af1d908-3c53-438b-bf71-023b6ace7f49\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre><__main__.DenseTransformer object at 0x00000137F508FBE0></pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"56a1b6ed-3e27-4581-b2da-378d4336fb89\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"56a1b6ed-3e27-4581-b2da-378d4336fb89\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f698d75a-3958-48b4-9eda-6c61469a2ef7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f698d75a-3958-48b4-9eda-6c61469a2ef7\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre><keras.wrappers.scikit_learn.KerasClassifier object at 0x00000137F508FFA0></pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=<generator object _BaseKFold.split at 0x000001370D0CD660>,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(max_features=3000)),\n",
       "                                       ('sparse_to_dense',\n",
       "                                        <__main__.DenseTransformer object at 0x00000137F508FBE0>),\n",
       "                                       ('scaler', MaxAbsScaler()),\n",
       "                                       ('kc',\n",
       "                                        <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000137F508FFA0>)]),\n",
       "             param_grid={'kc__batch_size': [16, 32, 50],\n",
       "                         'kc__dropout': [0.3, 0.1, 0],\n",
       "                         'kc__epochs': [20, 30, 50],\n",
       "                         'kc__neurons': [10, 20, 30, 100],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3f313",
   "metadata": {},
   "source": [
    "#### Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa750ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit = 100\n",
    "# X_p = prep_pipeline.fit_transform(X[:limit]).toarray()\n",
    "# X_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.multiclass import type_of_target\n",
    "# type_of_target(y) , type_of_target(yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0967873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = prep_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a56cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Tuning Time s: 4590.74\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kc__batch_size</th>\n",
       "      <th>param_kc__dropout</th>\n",
       "      <th>param_kc__epochs</th>\n",
       "      <th>param_kc__neurons</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2.892612</td>\n",
       "      <td>0.029515</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.772436</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.758120</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3.350023</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.392158</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'kc__batch_size': 32, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.771795</td>\n",
       "      <td>0.754487</td>\n",
       "      <td>0.754915</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3.014429</td>\n",
       "      <td>0.104706</td>\n",
       "      <td>0.374790</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.767308</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.752991</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3.782510</td>\n",
       "      <td>0.124156</td>\n",
       "      <td>0.514090</td>\n",
       "      <td>0.034297</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'kc__batch_size': 50, 'kc__dropout': 0.1, 'kc...</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>0.766026</td>\n",
       "      <td>0.751282</td>\n",
       "      <td>0.752564</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2.520893</td>\n",
       "      <td>0.052761</td>\n",
       "      <td>0.356069</td>\n",
       "      <td>0.024609</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'kc__batch_size': 50, 'kc__dropout': 0, 'kc__...</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.762179</td>\n",
       "      <td>0.757051</td>\n",
       "      <td>0.752564</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "146       2.892612      0.029515         0.380000        0.009401   \n",
       "74        3.350023      0.034255         0.392158        0.019389   \n",
       "148       3.014429      0.104706         0.374790        0.008012   \n",
       "169       3.782510      0.124156         0.514090        0.034297   \n",
       "194       2.520893      0.052761         0.356069        0.024609   \n",
       "\n",
       "    param_kc__batch_size param_kc__dropout param_kc__epochs param_kc__neurons  \\\n",
       "146                   50               0.3               20                20   \n",
       "74                    32               0.3               20                20   \n",
       "148                   50               0.3               20                30   \n",
       "169                   50               0.1               20                10   \n",
       "194                   50                 0               20                20   \n",
       "\n",
       "    param_tfidf__ngram_range  \\\n",
       "146                   (1, 1)   \n",
       "74                    (1, 1)   \n",
       "148                   (1, 1)   \n",
       "169                   (1, 2)   \n",
       "194                   (1, 1)   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "146  {'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...           0.743590   \n",
       "74   {'kc__batch_size': 32, 'kc__dropout': 0.3, 'kc...           0.738462   \n",
       "148  {'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...           0.738462   \n",
       "169  {'kc__batch_size': 50, 'kc__dropout': 0.1, 'kc...           0.740385   \n",
       "194  {'kc__batch_size': 50, 'kc__dropout': 0, 'kc__...           0.738462   \n",
       "\n",
       "     split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "146           0.772436           0.758333         0.758120        0.011777   \n",
       "74            0.771795           0.754487         0.754915        0.013612   \n",
       "148           0.767308           0.753205         0.752991        0.011777   \n",
       "169           0.766026           0.751282         0.752564        0.010507   \n",
       "194           0.762179           0.757051         0.752564        0.010189   \n",
       "\n",
       "     rank_test_score  \n",
       "146                1  \n",
       "74                 2  \n",
       "148                3  \n",
       "169                4  \n",
       "194                4  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "t0 = time.time()\n",
    "grid_fitted = grid.fit(X_t,y) # Pipe line fitted with preprocessed clean text spacy\n",
    "results  =  pd.DataFrame(grid_fitted.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "t1 = time.time()\n",
    "delta = t1-t0\n",
    "print(f'Tuning Time s: {round(delta,3)}')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e267f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24561bf4",
   "metadata": {},
   "source": [
    "## Fit the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4fdad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SAMPLE\n",
    "# limit = 1000\n",
    "\n",
    "# model = create_model(neurons=20)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0bef6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2201156616210938"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#+++++++++++++++++++++++ BEST PIPE PARAMS ++++++++++++++++++++++++++++++++\n",
    "opt_pipeline  = grid_fitted.best_estimator_\n",
    "\n",
    "t0 = time.time()\n",
    "fitted_pipe = opt_pipeline.fit(X_t,y)\n",
    "time.time() - t0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41a2a1",
   "metadata": {},
   "source": [
    "# Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21741706",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++Macro Categories++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "cat_dict = {\n",
    "**dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "**dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "**dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "**dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "**dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "**dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "**dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "**dict.fromkeys( ['sci.med'],'medicine') \n",
    "}\n",
    "df['label']=df['label'].map(cat_dict)\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "# Encode labels in column 'species'.\n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# drop non in categories\n",
    "df = df.loc[df['label'].isin(key_categories)]\n",
    "#++++++++++++++++++++++++ PICK RANDOM 30 % OF TEST++++++++++++++++++++++++++\n",
    "df = df.sample(frac = 1) \n",
    "# dependent and independent variable\n",
    "X_test = df['text']\n",
    "y_test = df['target']\n",
    "#_____________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc205b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = prep_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33dc05ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.78      0.65       794\n",
      "           1       0.88      0.74      0.80      1955\n",
      "           2       0.65      0.73      0.69       396\n",
      "           3       0.74      0.70      0.72      1050\n",
      "           4       0.71      0.77      0.74       649\n",
      "           5       0.57      0.83      0.68       390\n",
      "           6       0.69      0.61      0.65      1183\n",
      "           7       0.90      0.83      0.86       796\n",
      "\n",
      "    accuracy                           0.73      7213\n",
      "   macro avg       0.71      0.75      0.72      7213\n",
      "weighted avg       0.75      0.73      0.74      7213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred  = opt_pipeline.predict(X_test_prep)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12e3ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 {color: black;background-color: white;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 pre{padding: 0;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-toggleable {background-color: white;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-estimator:hover {background-color: #d4ebff;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-item {z-index: 1;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-parallel-item:only-child::after {width: 0;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-73e59af2-cbc2-4dab-ab8e-6de138578826 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-73e59af2-cbc2-4dab-ab8e-6de138578826\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"dca438d3-c506-46aa-b181-18f090cc35f6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"dca438d3-c506-46aa-b181-18f090cc35f6\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('tfidf', TfidfVectorizer(max_features=3000)),\n",
       "                ('sparse_to_dense',\n",
       "                 <__main__.DenseTransformer object at 0x00000137CFF15A60>),\n",
       "                ('scaler', MaxAbsScaler()),\n",
       "                ('kc',\n",
       "                 <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001378CEEEFA0>)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bd13f6dc-4194-4d00-9338-6aab7371b4a9\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bd13f6dc-4194-4d00-9338-6aab7371b4a9\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=3000)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2b3af973-5579-4dbc-abcb-bee03534a4a7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2b3af973-5579-4dbc-abcb-bee03534a4a7\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre><__main__.DenseTransformer object at 0x00000137CFF15A60></pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fc5eb979-21ee-4ce4-a0e1-9095356f80f4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fc5eb979-21ee-4ce4-a0e1-9095356f80f4\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b6e528da-9af3-494d-8272-5afb072ae589\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b6e528da-9af3-494d-8272-5afb072ae589\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre><keras.wrappers.scikit_learn.KerasClassifier object at 0x000001378CEEEFA0></pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_features=3000)),\n",
       "                ('sparse_to_dense',\n",
       "                 <__main__.DenseTransformer object at 0x00000137CFF15A60>),\n",
       "                ('scaler', MaxAbsScaler()),\n",
       "                ('kc',\n",
       "                 <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001378CEEEFA0>)])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_proba_pred = pd.DataFrame(opt_pipeline.predict_proba(X_test_prep), columns=key_categories)\n",
    "\n",
    "dist_proba_pred.hist(figsize = (10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e216f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_predictions(fitted_pipe):return label_encoder.inverse_transform(fitted_pipe.predict(pd.Series(input('Input-Text:'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51570d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_custom_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_pipe(pipeline,name_model = 'keras_model.h5', name_pipe ='sklearn_pipeline.pkl' ):\n",
    "    # Save the Keras model first:\n",
    "    pipeline.named_steps['kc'].model.save(name_model)\n",
    "\n",
    "    # This hack allows us to save the sklearn pipeline:\n",
    "    pipeline.named_steps['kc'].model = None\n",
    "\n",
    "    # Finally, save the pipeline:\n",
    "    joblib.dump(pipeline, name_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dacb2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keras_pipe(fitted_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c1617",
   "metadata": {},
   "source": [
    "# Load the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f6d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++Macro Categories++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "cat_dict = {\n",
    "**dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "**dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "**dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "**dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "**dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "**dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "**dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "**dict.fromkeys( ['sci.med'],'medicine') \n",
    "}\n",
    "df['label']=df['label'].map(cat_dict)\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "# Encode labels in column 'species'.\n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# drop non in categories\n",
    "df = df.loc[df['label'].isin(key_categories)]\n",
    "#++++++++++++++++++++++++ PICK RANDOM 30 % OF TEST++++++++++++++++++++++++++\n",
    "df = df.sample(frac = 1) \n",
    "# dependent and independent variable\n",
    "X_test = df['text']\n",
    "y_test = df['target']\n",
    "#_____________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pipeline first:\n",
    "pipeline = joblib.load('sklearn_pipeline.pkl')\n",
    "\n",
    "# Then, load the Keras model:\n",
    "pipeline.named_steps['kc'].model = load_model('keras_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e80cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "X_test_prep = prep_pipeline.transform(X_test)\n",
    "y_pred  = pipeline.predict(X_test_prep)\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(14, 12, forward=True)\n",
    "#fig.align_labels()\n",
    "\n",
    "label_names = df['label'].unique()\n",
    "\n",
    "print(classification_report(y_test,y_pred, target_names=label_names))\n",
    "# fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.asarray(label_names), normalize=False,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280809a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_custom_predictions(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bc060",
   "metadata": {},
   "source": [
    "# Embedding model with Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7369f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserire flag di dominio dummy \n",
    "\n",
    "# def spacy_tokenizer(document, nlp=nlp):\n",
    "#     # tokenize the document with spaCY\n",
    "#     doc = nlp(document)\n",
    "#     # Remove stop words and punctuation symbols\n",
    "#     tokens = [\n",
    "#         token.text for token in doc if (\n",
    "#         token.is_stop == False and \\\n",
    "#         token.is_punct == False and \\\n",
    "#         token.text.strip() != '' and \\\n",
    "#         token.text.find(\"\\n\") == -1)]\n",
    "#     return tokens\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "# load embeddings model from Tensorflow Hub\n",
    "#https://stackoverflow.com/questions/62464152/universal-sentence-encoder-load-error-error-savedmodel-file-does-not-exist-at\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "\n",
    "matrix_features  = 512\n",
    "\n",
    "n_classes =y.nunique()\n",
    "def create_model(optimizer=\"adam\",\n",
    "                 dense_layer_sizes = False,\n",
    "                 dropout=0.1, init='uniform',\n",
    "                 features=matrix_features,neurons=20,\n",
    "                 n_classes = n_classes ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(features,),kernel_initializer=init)) #\n",
    "    model.add(Dropout(dropout), )    \n",
    "\n",
    "    #for layer_size in dense_layer_sizes:\n",
    "    #   model.add(Dense(layer_size, activation='relu'))\n",
    "    #   model.add(Dropout(dropout), )    \n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['sparse_categorical_crossentropy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed(pd.Series(spacy_preprocessing(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e044110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = prep_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9284603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_2356/2802624789.py:12: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  ('kc' ,KerasClassifier(build_fn=create_model, verbose = 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Tuning Time s: 61142.078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.289283990859985"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return embed(X).numpy()\n",
    "    \n",
    "\n",
    "emb_pipeline = Pipeline([\n",
    "                    ('embed', Embedding()),\n",
    "                    ('kc' ,KerasClassifier(build_fn=create_model, verbose = 0))\n",
    "])\n",
    "\n",
    "#++++++++++++++++++++++++++++++ GRID ++++++++++++++++++++++++++++++++++++++\n",
    "param_grid = {\n",
    "    'kc__epochs': [20,30,50],\n",
    "    'kc__neurons': [10, 20, 30, 100],\n",
    "    'kc__batch_size':[16, 32,50],\n",
    "    'kc__dropout': [ 0.3, 0.1, 0]\n",
    "}\n",
    "\n",
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "grid = GridSearchCV(estimator=emb_pipeline,\n",
    "                    verbose=1,\n",
    "                    cv=skf.split(X_t,y),\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy')\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "t0 = time.time()\n",
    "grid_fitted = grid.fit(X_t,y) # Pipe line fitted with preprocessed clean text spacy\n",
    "results  =  pd.DataFrame(grid_fitted.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "t1 = time.time()\n",
    "delta = t1-t0\n",
    "print(f'Tuning Time s: {round(delta,3)}')\n",
    "results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "355a39af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kc__batch_size</th>\n",
       "      <th>param_kc__dropout</th>\n",
       "      <th>param_kc__epochs</th>\n",
       "      <th>param_kc__neurons</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>14.354582</td>\n",
       "      <td>0.288370</td>\n",
       "      <td>5.656976</td>\n",
       "      <td>0.277271</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>{'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.792308</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.795513</td>\n",
       "      <td>0.797436</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>13.186686</td>\n",
       "      <td>0.874001</td>\n",
       "      <td>4.861567</td>\n",
       "      <td>0.382476</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>{'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.796154</td>\n",
       "      <td>0.797436</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.765772</td>\n",
       "      <td>0.259575</td>\n",
       "      <td>5.286987</td>\n",
       "      <td>0.783942</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>{'kc__batch_size': 16, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.789103</td>\n",
       "      <td>0.801923</td>\n",
       "      <td>0.796795</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14.323750</td>\n",
       "      <td>0.115464</td>\n",
       "      <td>5.758810</td>\n",
       "      <td>0.835035</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>{'kc__batch_size': 32, 'kc__dropout': 0.3, 'kc...</td>\n",
       "      <td>0.791026</td>\n",
       "      <td>0.797436</td>\n",
       "      <td>0.796795</td>\n",
       "      <td>0.795085</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>13.806980</td>\n",
       "      <td>0.286871</td>\n",
       "      <td>5.249622</td>\n",
       "      <td>0.552629</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>{'kc__batch_size': 32, 'kc__dropout': 0.1, 'kc...</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.796795</td>\n",
       "      <td>0.796154</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "74      14.354582      0.288370         5.656976        0.277271   \n",
       "78      13.186686      0.874001         4.861567        0.382476   \n",
       "1       13.765772      0.259575         5.286987        0.783942   \n",
       "38      14.323750      0.115464         5.758810        0.835035   \n",
       "53      13.806980      0.286871         5.249622        0.552629   \n",
       "\n",
       "   param_kc__batch_size param_kc__dropout param_kc__epochs param_kc__neurons  \\\n",
       "74                   50               0.3               20                30   \n",
       "78                   50               0.3               30                30   \n",
       "1                    16               0.3               20                20   \n",
       "38                   32               0.3               20                30   \n",
       "53                   32               0.1               30                20   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "74  {'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...           0.792308   \n",
       "78  {'kc__batch_size': 50, 'kc__dropout': 0.3, 'kc...           0.788462   \n",
       "1   {'kc__batch_size': 16, 'kc__dropout': 0.3, 'kc...           0.789103   \n",
       "38  {'kc__batch_size': 32, 'kc__dropout': 0.3, 'kc...           0.791026   \n",
       "53  {'kc__batch_size': 32, 'kc__dropout': 0.1, 'kc...           0.791667   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "74           0.804487           0.795513         0.797436        0.005155   \n",
       "78           0.807692           0.796154         0.797436        0.007903   \n",
       "1            0.801923           0.796795         0.795940        0.005269   \n",
       "38           0.797436           0.796795         0.795085        0.002883   \n",
       "53           0.796795           0.796154         0.794872        0.002281   \n",
       "\n",
       "    rank_test_score  \n",
       "74                1  \n",
       "78                1  \n",
       "1                 3  \n",
       "38                4  \n",
       "53                5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "070dd692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.40979266166687"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#+++++++++++++++++++++++ BEST PIPE PARAMS ++++++++++++++++++++++++++++++++\n",
    "opt_pipeline  = grid_fitted.best_estimator_\n",
    "\n",
    "t0 = time.time()\n",
    "fitted_pipe_emb = opt_pipeline.fit(X_t,y)\n",
    "time.time() - t0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4cf574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = prep_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59e66277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       794\n",
      "           1       0.90      0.76      0.82      1955\n",
      "           2       0.70      0.84      0.76       396\n",
      "           3       0.81      0.76      0.78      1050\n",
      "           4       0.79      0.79      0.79       649\n",
      "           5       0.54      0.79      0.64       390\n",
      "           6       0.62      0.69      0.66      1183\n",
      "           7       0.88      0.88      0.88       796\n",
      "\n",
      "    accuracy                           0.77      7213\n",
      "   macro avg       0.76      0.79      0.77      7213\n",
      "weighted avg       0.79      0.77      0.78      7213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred  = fitted_pipe_emb.predict(X_test_prep)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1eb736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_328\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_656 (Dense)           (None, 30)                15390     \n",
      "                                                                 \n",
      " dropout_328 (Dropout)       (None, 30)                0         \n",
      "                                                                 \n",
      " dense_657 (Dense)           (None, 8)                 248       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,638\n",
      "Trainable params: 15,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_model(init='uniform', features=512, neurons=30, n_classes=8, dropout=0.3).summary()  #batchsize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a601b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pipe_emb.named_steps['kc'].model.save('keras_model_emb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2475c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input-Text: The final score of the game is 1:0 for the red team\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sport'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_custom_predictions(fitted_pipe_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa4e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
