{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf541617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import re\n",
    "from string import punctuation\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91421d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_w ='C:/Users/david/Desktop/'\n",
    "news_folder = '20_newsgroup/'\n",
    "my_path  = root_w+news_folder \n",
    "\n",
    "#List of folder names to make valid pathnames later\n",
    "folders = [f for f in os.listdir(my_path)]\n",
    "#2D list to store list of all files in different folders\n",
    "\n",
    "files = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    files.append([f for f in os.listdir(folder_path)])\n",
    "    \n",
    "#Create a list to each document\n",
    "pathname_list = []\n",
    "for fo in range(len(folders)):\n",
    "    for fi in files[fo]:\n",
    "        pathname_list.append(join(my_path, join(folders[fo], fi)))\n",
    "        \n",
    "#Create Target for each documents\n",
    "Y = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    num_of_files= len(os.listdir(folder_path))\n",
    "    for i in range(num_of_files):\n",
    "        Y.append(folder_name)\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "doc_train, doc_test, Y_train, Y_test = train_test_split(pathname_list, Y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a0a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e539b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb01307",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "cat_dict = {\n",
    "**dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "**dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "**dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "**dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "**dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "**dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "**dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "**dict.fromkeys( ['sci.med'],'medicine') \n",
    "}\n",
    "\n",
    "df['label']=df['label'].map(cat_dict)\n",
    "\n",
    "\n",
    "\n",
    "df['Number_of_words'] = df['text'].apply(lambda x:len(str(x).split()))\n",
    "\n",
    "\n",
    "no_text = df[df['Number_of_words']==0]\n",
    "print('No text records: ',len(no_text))\n",
    "\n",
    "# drop these rows\n",
    "df.drop(no_text.index,inplace=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda x:tokenizer.tokenize(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    custom_no_contest = ['subject:','from:', 'date:', 'newsgroups:', 'message-id:', 'lines:', 'path:', 'organization:', \n",
    "                        'would', 'writes:', 'references:', 'article', 'sender:', 'nntp-posting-host:', 'people', \n",
    "                        'university', 'think', 'xref:', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution:', 'first', \n",
    "                        'anyone','world', 'really', 'since', 'right', 'believe', 'still', \n",
    "                        \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\"]\n",
    "    stopWords = stopwords.words('english')\n",
    "    stopWords+=custom_no_contest\n",
    "    words = [w for w in text if w not in stopWords ]\n",
    "    return words \n",
    "\n",
    "df['stopwordremove_tokens'] = df['tokens'].apply(lambda x : remove_stopwords(x))\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "def lem_word(x):\n",
    "    return [lem.lemmatize(w) for w in x]\n",
    "\n",
    "df['lemmatized_text'] = df['stopwordremove_tokens'].apply(lem_word)\n",
    "\n",
    "\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "df['final_text'] = df['lemmatized_text'].apply(lambda x : combine_text(x))\n",
    "\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "  \n",
    "# Encode labels in column 'species'.\n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "\n",
    "# dependent and independent variable\n",
    "X = df['final_text']\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc5f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    doc = text\n",
    "    tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens  = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens  if len(token)> 3 and len(token) < 10]\n",
    "    clean_tokens = remove_stopwords(tokens)\n",
    "    nlp_text = combine_text(clean_tokens)\n",
    "    return nlp_text\n",
    "\n",
    "\n",
    "def spacy_preprocessing(text_format):\n",
    "    # Handle multiple input str pandas array\n",
    "    if not isinstance(text_format,str):\n",
    "        doc  = nlp(text_format.item())\n",
    "    else:\n",
    "        doc  = nlp(text_format)\n",
    "        \n",
    "    tokens_list  = [ token.lemma_ for token in doc if not token.is_punct and not token.is_space and token.is_alpha]\n",
    "    filter_token_sw = [token for token in tokens_list if token not in stopwords.words('english')]\n",
    "    return combine_text(filter_token_sw)\n",
    "\n",
    "pipe_spacy_preprocessing = FunctionTransformer(spacy_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f103655",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.sample(1).text.item()\n",
    "text_preprocessing(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e08c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_preprocessing(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda91c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef638f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import re\n",
    "from string import punctuation\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras import Sequential\n",
    "\n",
    "\n",
    "def vectorized_spacy_preprocessing(data):\n",
    "    \n",
    "    def spacy_preprocessing(text_format):\n",
    "        \n",
    "        def combine_text(list_of_text):\n",
    "            combined_text = ' '.join(list_of_text)\n",
    "            return combined_text\n",
    "\n",
    "        doc  = nlp(text_format)\n",
    "\n",
    "        tokens_list  = [ token.lemma_ for token in doc if not token.is_punct and not token.is_space and token.is_alpha]\n",
    "        filter_token_sw = [token for token in tokens_list if token not in stopwords.words('english')]\n",
    "        #print('Done')\n",
    "        return combine_text(filter_token_sw)\n",
    "\n",
    "    return list(map(spacy_preprocessing,data))\n",
    "\n",
    "\n",
    "\n",
    "#______________________________________________________ DATA INGESTION___________________________________________________________________\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset.data\n",
    "df['source'] = dataset.target\n",
    "label=[]\n",
    "for i in df['source']:\n",
    "    label.append(dataset.target_names[i])\n",
    "df['label']=label\n",
    "df.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# key_categories = ['politics','sport','religion','computer','sales','automobile','science','medicine']\n",
    "# cat_dict = {\n",
    "# **dict.fromkeys(['talk.politics.misc','talk.politics.guns','talk.politics.mideast'],'politics'),\n",
    "# **dict.fromkeys( ['rec.sport.hockey','rec.sport.baseball'],'sport'),\n",
    "# **dict.fromkeys( ['soc.religion.christian','talk.religion.misc'],'religion'),\n",
    "# **dict.fromkeys(['comp.windows.x','comp.sys.ibm.pc.hardware','comp.os.ms-windows.misc','comp.graphics','comp.sys.mac.hardware'],'computer'),\n",
    "# **dict.fromkeys( ['misc.forsale'],'sales'),\n",
    "# **dict.fromkeys( ['rec.autos','rec.motorcycles'],'automobile'),\n",
    "# **dict.fromkeys( ['sci.crypt','sci.electronics','sci.space'],'science'),\n",
    "# **dict.fromkeys( ['sci.med'],'medicine') \n",
    "# }\n",
    "# df['label']=df['label'].map(cat_dict)\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "# Encode labels in column 'species'.\n",
    "df['target']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "\n",
    "# dependent and independent variable\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "#_____________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5eb133",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e7602196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer=\"adam\", dropout=0.1, init='uniform', nbr_features=2500, dense_nparams=256,n_classes = len(set(label) ) ):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_nparams, activation='relu', input_shape=(nbr_features,), kernel_initializer=init,)) \n",
    "    model.add(Dropout(dropout), )\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=[metrics.AUC])\n",
    "    return model\n",
    "\n",
    "kears_estimator = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "clf = KerasClassifier(build_fn=create_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010bebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26d603d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "  'tfidf__ngram_range': [(1,1), (1,2), (2,2), (1,3)],\n",
    "  'tfidf__use_idf': [True, False],\n",
    "}]\n",
    "\n",
    "\n",
    "pipe_spacy_preprocessing = FunctionTransformer(vectorized_spacy_preprocessing)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                    ('text_preprocessing', pipe_spacy_preprocessing ),\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                    ('classifier',clf)\n",
    "                    ])\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, scoring=make_scorer(f1_score , average='micro') , cv=StratifiedKFold(n_splits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce18a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "grid_results  = grid_search.fit(X[:10].to,y[:10])\n",
    "\n",
    "time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "results  =  pd.DataFrame(grid_results.cv_results_).sort_values('rank_test_score')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a85d7644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('text_preprocessing',\n",
       "                 FunctionTransformer(func=<function vectorized_spacy_preprocessing at 0x000002AF515DEDC0>)),\n",
       "                ('tfidf', TfidfVectorizer()),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e6b565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f200ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4425967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
